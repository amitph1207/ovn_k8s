# 2 testcases:
1) pod to pod on same node
2) pod to pod on different node

kubectl get pods 
NAME        READY   STATUS    RESTARTS        AGE
busybox     1/1     Running   1 (5m31s ago)   65m
busybox-2   1/1     Running   0               54m
busybox-3   1/1     Running   0               3m39s


kubectl exec -it busybox -- ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
9: veth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue 
    link/ether 00:02:00:00:00:34 brd ff:ff:ff:ff:ff:ff
    inet 10.100.1.52/24 brd 10.100.1.255 scope global veth0
       valid_lft forever preferred_lft forever
    inet6 fe80::d4bb:5dff:fe07:6014/64 scope link 
       valid_lft forever preferred_lft forever
laborant@dev-machine:~$ 
laborant@dev-machine:~$ 
laborant@dev-machine:~$ kubectl exec -it busybox-3 -- ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
10: veth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue 
    link/ether 00:02:00:00:00:96 brd ff:ff:ff:ff:ff:ff
    inet 10.100.1.150/24 brd 10.100.1.255 scope global veth0
       valid_lft forever preferred_lft forever
    inet6 fe80::54cf:b8ff:fe51:6c16/64 scope link 
       valid_lft forever preferred_lft forever
laborant@dev-machine:~$ kubectl exec -it busybox-2 -- ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
11: veth0@if10: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue 
    link/ether 00:02:00:00:00:4d brd ff:ff:ff:ff:ff:ff
    inet 10.100.1.77/24 brd 10.100.1.255 scope global veth0
       valid_lft forever preferred_lft forever
    inet6 fe80::b45f:98ff:fe50:9fdc/64 scope link 
       valid_lft forever preferred_lft forever


#ping on different node -> from busybox-3 to busybox-2 and 1 (node2 -> node1)
  
laborant@dev-machine:~$ kubectl exec -it busybox-3 -- ping 10.100.1.77
PING 10.100.1.77 (10.100.1.77): 56 data bytes
64 bytes from 10.100.1.77: seq=0 ttl=64 time=0.771 ms
64 bytes from 10.100.1.77: seq=1 ttl=64 time=1.268 ms
^C
--- 10.100.1.77 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.771/1.019/1.268 ms
laborant@dev-machine:~$ kubectl exec -it busybox-3 -- ping 10.100.1.52
PING 10.100.1.52 (10.100.1.52): 56 data bytes
64 bytes from 10.100.1.52: seq=0 ttl=64 time=0.970 ms
64 bytes from 10.100.1.52: seq=1 ttl=64 time=1.349 ms
^C
--- 10.100.1.52 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.970/1.159/1.349 ms


# ping on same node -> from busybox to busybox-2

kubectl exec -it busybox -- ping 10.100.1.77
PING 10.100.1.77 (10.100.1.77): 56 data bytes
64 bytes from 10.100.1.77: seq=0 ttl=64 time=0.037 ms
64 bytes from 10.100.1.77: seq=1 ttl=64 time=0.130 ms
^C
--- 10.100.1.77 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.037/0.083/0.130 ms


# ovn logs:

# control plane:

root@cplane-01:laborant# ovn-sbctl show
Chassis "aa7691e1-a019-4909-a4e7-646bae44e4a7"
    hostname: node-02
    Encap geneve
        ip: "172.16.0.4"
        options: {csum="true"}
    Port_Binding pod-c7aa0dbb21df
Chassis "5d53d36e-cc2c-4f28-8d1b-28f2ceaf08b9"
    hostname: node-01
    Encap geneve
        ip: "172.16.0.3"
        options: {csum="true"}
    Port_Binding pod-ef53fb8ca412
    Port_Binding pod-3b913ad05992
root@cplane-01:laborant# 
root@cplane-01:laborant# 
root@cplane-01:laborant# ovn-nbctl show 
switch 0d99ce63-41ae-4ad5-91c6-d66be6e251bb (ls1)
    port pod-ef53fb8ca412
        addresses: ["00:02:00:00:00:34 10.100.1.52"]
    port pod-3b913ad05992
        addresses: ["00:02:00:00:00:4D 10.100.1.77"]
    port pod-c7aa0dbb21df
        addresses: ["00:02:00:00:00:96 10.100.1.150"]


# data plane:

ovs-vsctl show
12ba2842-277b-4d58-9287-a7df5b6bd862
    Bridge br-int
        fail_mode: secure
        datapath_type: system
        Port br-int
            Interface br-int
                type: internal
        Port veth3b913ad0
            Interface veth3b913ad0
        Port vethef53fb8c
            Interface vethef53fb8c
        Port ovn-aa7691-0
            Interface ovn-aa7691-0
                type: geneve
                options: {csum="true", key=flow, remote_ip="172.16.0.4"}
    ovs_version: "3.3.4"
root@node-01:laborant# ovs-ofctl dump-ports br-int
OFPST_PORT reply (xid=0x2): 4 ports
  port LOCAL: rx pkts=0, bytes=0, drop=0, errs=0, frame=0, over=0, crc=0
           tx pkts=0, bytes=0, drop=0, errs=0, coll=0
  port  vethef53fb8c: rx pkts=1062, bytes=102016, drop=0, errs=0, frame=0, over=0, crc=0
           tx pkts=1094, bytes=104260, drop=0, errs=0, coll=0
  port  veth3b913ad0: rx pkts=1063, bytes=102058, drop=0, errs=0, frame=0, over=0, crc=0
           tx pkts=1076, bytes=103044, drop=0, errs=0, coll=0
  port  "ovn-aa7691-0": rx pkts=18, bytes=1364, drop=?, errs=?, frame=?, over=?, crc=?
           tx pkts=4, bytes=392, drop=?, errs=?, coll=?